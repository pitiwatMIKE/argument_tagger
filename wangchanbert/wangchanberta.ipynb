{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "batch_size = 12\n",
    "label_list = ['O', 'B-c', 'I-c', 'B-p', 'I-p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n",
      "[('ถ้า', 'JSBR', 'B-c'), ('เดินทาง', 'VACT', 'I-c'), ('กลางคืน', 'NCMN', 'I-c'), ('ก็', 'JSBR', 'I-c'), ('รถทัวร์', 'NCMN', 'I-c'), ('ครับ', 'NCMN', 'I-c'), (' ', 'PUNC', 'I-c'), (' ', 'PUNC', 'O'), ('เพราะ', 'JSBR', 'B-p'), ('รถ', 'NCMN', 'I-p'), ('ไม่', 'NEG', 'I-p'), ('เยอะ', 'VSTA', 'I-p'), (' ', 'PUNC', 'I-p'), ('ความเสี่ยง', 'NCMN', 'I-p'), ('การ', 'FIXN', 'I-p'), ('เกิด', 'VSTA', 'I-p'), ('อุบัติ', 'NCMN', 'I-p'), ('ห', 'NCMN', 'I-p'), ('ตุ', 'NCMN', 'I-p'), ('ก็', 'JSBR', 'I-p'), ('น้อย', 'VATT', 'I-p'), ('(', 'PUNC', 'I-p'), ('มั้ง', 'JCRG', 'I-p'), (')', 'PUNC', 'I-p'), (' ', 'PUNC', 'I-p'), (' ', 'PUNC', 'O'), ('ถ้า', 'JSBR', 'B-c'), ('กลางวัน', 'NCMN', 'I-c'), ('ก็', 'JSBR', 'I-c'), ('เครื่องบิน', 'VSTA', 'I-c'), ('ครับ', 'NCMN', 'I-c'), (' ', 'PUNC', 'I-c'), (' ', 'PUNC', 'O'), ('เพราะ', 'JSBR', 'B-p'), (' ', 'PUNC', 'I-p'), ('มัน', 'PPRS', 'I-p'), ('ใช้เวลา', 'VSTA', 'I-p'), ('น้อย', 'ADVN', 'I-p'), ('จะ', 'XVBM', 'I-p'), ('ได้', 'XVAM', 'I-p'), ('มี', 'VSTA', 'I-p'), ('เวลา', 'NCMN', 'I-p'), ('ระหว่าง', 'RPRE', 'I-p'), ('วัน', 'NCMN', 'I-p'), ('เยอะ', 'NCMN', 'I-p'), ('ๆ', 'PUNC', 'I-p'), (' ', 'PUNC', 'I-p')]\n"
     ]
    }
   ],
   "source": [
    "path_name = \"../dataset/data/\"\n",
    "\n",
    "with open(path_name + 'comment-pos.data', 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile:\n",
    "    text_inside = []\n",
    "    for word, pos, label in data:\n",
    "        if word.strip() == '':\n",
    "            text_inside.append((' ', pos, label))\n",
    "        else:\n",
    "            text_inside.append((word, pos, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(data):\n",
    "    datasets = dict()\n",
    "    datasets['id'] = []\n",
    "    datasets['tokens'] = []\n",
    "    datasets['pos_tags'] = []\n",
    "    datasets['ner_tags'] = []\n",
    "    for i, sent in enumerate(data):\n",
    "        _word = []\n",
    "        _label = []\n",
    "        _pos = []\n",
    "        for word, pos, label in sent:\n",
    "            _word.append(word)\n",
    "            _label.append(label)\n",
    "            _pos.append(pos)\n",
    "        datasets['id'].append(i)\n",
    "        datasets['tokens'].append(_word)\n",
    "        datasets['pos_tags'].append(_pos)\n",
    "        datasets['ner_tags'].append([label_list.index(l) for l in _label])\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitiw\\miniconda3\\envs\\wangchan\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 552\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset,Sequence, Features, Value, ClassLabel\n",
    "\n",
    "ft = Features({\n",
    "    'id': Value(\"int32\"),\n",
    "    'tokens': Sequence(Value(\"string\")), \n",
    "    'pos_tags': Sequence(Value(\"string\")),\n",
    "    'ner_tags': Sequence(ClassLabel(names=label_list))\n",
    "    })\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict(data_to_dict(train_sents), features=ft),\n",
    "    'test': Dataset.from_dict(data_to_dict(test_sents), features=ft)\n",
    "})\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 5\n",
      "['O', 'B-c', 'I-c', 'B-p', 'I-p']\n"
     ]
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "print('len:', len(label_list))\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291</td>\n",
       "      <td>[โดยส่วนตัว, แล้ว, อยาก, ให้, สถานศึกษา, เป็น, คน, กำหนด, มากกว่า,  , เพราะว่า, ถ้า, คุณ, ต้องการ, จะ, ศึกษา, ที่, โรงเรียน, ไหน, ๆ,  , คุณ, ควรจะ, ยอมรับ, กฎเกณฑ์, ของ, แต่ละ, โรงเรียน, นั้น, ได้,  , ใช่, ค่ะ,  , เครื่องแบบ, ไม่, ได้, เป็นความ, บาง, ชี้, ถึง, ผลการเรียน,  , อย่างไรก็ตาม, คุณ, ต้อง, ยอมรับ, กฎ, ของ, สถานศึกษา, ถ้า, คุณ, อยาก, เรียน, ก็, ต้อง, ทำตาม, กฎ, ของ, โรงเรียน, ค่ะ]</td>\n",
       "      <td>[NCMN, XVAE, XVAM, VACT, NCMN, VSTA, NCMN, VACT, ADVN, PUNC, JSBR, JSBR, PPRS, VACT, XVBM, VACT, PREL, NCMN, PNTR, NCMN, PUNC, PPRS, XVMM, VACT, NCMN, RPRE, DIBQ, NCMN, DDAC, XVAE, PUNC, VSTA, NCMN, PUNC, NCMN, NEG, XVAE, VACT, DIBQ, VACT, RPRE, NCMN, PUNC, JSBR, VACT, XVMM, VACT, NCMN, RPRE, NCMN, JSBR, VACT, NPRP, VACT, JSBR, XVMM, VACT, NCMN, RPRE, NCMN, NCMN]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>350</td>\n",
       "      <td>[เกมส์, เป็น, แค่, สื่อ, ๆ, หนึ่ง,  , ไม่, ต่าง, จาก,  , หนัง,  , ละคร,  , รายการโทรทัศน์,  , เพลง,  , หรือ, มหสพ, ครับ,  , ใช้, ให้, ถูก, ทาง, คนใน,  , Silicon,  , Valley,  , ที่, เห็น, เป็น, ผู้ชาย, เยอะ, ๆ, เพราะ, เล่น, เกมส์, มา, ตั้งแต่, เด็ก, ๆ, ทั้งนั้น, ครับ,  , เด็กผู้หญิง, ใน, ยุค, 80, -, 90, เล่น, VDO, เกมส์, น้อยกว่า, มาก,  , มัน, เป็น, จุดเริ่มต้น, ของ, ความสนใจ, ใน, เรื่อง, เกี่ยวกับ,  , IT, ครับ]</td>\n",
       "      <td>[NCMN, VSTA, NCMN, NCMN, NCMN, DCNM, PUNC, NEG, VSTA, RPRE, PUNC, NCMN, PUNC, NCMN, PUNC, NCMN, PUNC, NCMN, PUNC, JCRG, NCMN, NCMN, PUNC, VACT, JSBR, VATT, NCMN, NCMN, PUNC, NCMN, PUNC, NCMN, PUNC, PREL, VSTA, VSTA, NCMN, NCMN, PUNC, JSBR, VACT, NCMN, XVAE, RPRE, NCMN, NCMN, DDAN, DCNM, PUNC, VACT, RPRE, NCMN, NCNM, PUNC, DCNM, VACT, NCMN, VSTA, ADVN, ADVN, NCMN, PPRS, VSTA, NCMN, RPRE, NCMN, RPRE, NCMN, RPRE, PUNC, NCMN, NCMN]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>382</td>\n",
       "      <td>[อ่าน,  , คห,  , 3,  , เขียน, ได้ดี,  , รู้, ลึก,  , รู้, จริง,  , ถ้า, จะ, ให้, ลูก, เรียน,  , ปวช., ใน, กรุงเทพ,  , ปริมณฑล,  , อย่า, เรียน,  , โรงเรียน, ที่, ชอบ, ตี, กัน,  , และ, เป็นข่าว,  , มี, โจทย์, เยอะ, จะ, หา, ความสบายใจ, ไม่, ได้, เลย,  , ไม่, รู้, ว่า, ลูก, จะ, กลับ, ถึง, บ้าน, หรือเปล่า,  , หรือ, จะ, นอน, อยู่, ข้าง, ถนน,  , นี่, คือ, ข้อเสีย, ของ, การเรียน, สาย, อาชีพ, ในประเทศ, ไทย,  , ทำให้, ไม่, อยาก, ให้, ลูก, เรียน]</td>\n",
       "      <td>[VACT, PUNC, NCMN, PUNC, DCNM, PUNC, VACT, NCMN, PUNC, VSTA, VATT, PUNC, VSTA, ADVN, PUNC, JSBR, XVBM, VACT, NCMN, VACT, PUNC, NCMN, RPRE, NCMN, PUNC, NCMN, PUNC, NCMN, VACT, PUNC, NCMN, PREL, VACT, VACT, ADVN, PUNC, JCRG, NCMN, PUNC, VSTA, NCMN, NCMN, XVBM, VACT, NCMN, NEG, XVAE, ADVN, PUNC, NEG, VSTA, JSBR, NCMN, XVBM, VACT, RPRE, NCMN, NCMN, PUNC, JCRG, XVBM, VSTA, XVAE, NCMN, NCMN, PUNC, PDMN, VSTA, NCMN, RPRE, NCMN, NCMN, NCMN, NCMN, NPRP, PUNC, VACT, NEG, XVMM, VACT, NCMN, VACT]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>458</td>\n",
       "      <td>[เลือก, เงิน, ค่ะ,  ,  , เพราะ, มี,  , 20,  , ล้าน, แล้ว, เดี๋ยว, หา, หนุ่ม, ๆ,  , มา, ดูแล, ได้, ไม่, ยาก, ค่ะ,  ]</td>\n",
       "      <td>[VACT, NCMN, NCMN, PUNC, PUNC, JSBR, VSTA, PUNC, DCNM, PUNC, CNIT, XVAE, VACT, VACT, NCMN, NCMN, PUNC, XVAE, VACT, XVAE, NEG, VATT, NCMN, PUNC]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>381</td>\n",
       "      <td>[ตอนนี้, ก็, ต้อง, สตาร์ท, มือ, แล้ว, ครับ,  , เพราะ, ข้อ, เข่า, ไม่, ดี,  ]</td>\n",
       "      <td>[JSBR, JSBR, XVMM, NCMN, NCMN, XVAE, VACT, PUNC, JSBR, NCMN, NCMN, NEG, VATT, PUNC]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, B-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>149</td>\n",
       "      <td>[เอา, จริงๆ,  , เรา, เคย, เป็น, เด็ก,  , (, ประถม, ),  , กรุงเทพ,  , ที่, ไป, เรียน, ที่, อีสาน, อยู่, ปี, นึง,  , (, ผ่าน, มา, นาน, แล้ว,  , เดี๋ยวนี้, อาจจะ, ไม่, เป็น, แบบนี้, แล้ว, ),  , คน, ตจ, ว.,  , หรือ, คน, อีสาน,  , เค้า, ก็, เหยียด, คน,  , กทม.,  , นะคะ,  , ว่า, เหยียบขี้ไก่ไม่ฝ่อ,  , ลำบาก, ไม่, เป็น,  , สบาย, จน, เคยตัว,  , อะไร, แบบนี้,  , ตอน, เด็ก, ๆ,  , เรา, ต้อง, ต่อสู้, กับ, คำพูด, แนว, ๆ,  , นี้, เยอะ, มาก, เหมือนกัน, ค่ะ,  , หลัก, ๆ,  , คือ, จะ, บอ, กว่า,  , การ, เหยียด, จะ, เกิดขึ้น, ต่อเมื่อ,  , ...]</td>\n",
       "      <td>[VACT, ADVI, PUNC, PPRS, XVMM, VSTA, NCMN, PUNC, PUNC, NCMN, PUNC, PUNC, NCMN, PUNC, PREL, VACT, VACT, PREL, VACT, XVAE, NCMN, ADVN, PUNC, PUNC, VSTA, XVAE, ADVN, XVAE, PUNC, JSBR, XVMM, NEG, VSTA, NCMN, XVAE, PUNC, PUNC, NCMN, NCMN, NCMN, PUNC, JCRG, NCMN, VACT, PUNC, NCMN, JSBR, VSTA, NCMN, PUNC, NCMN, PUNC, NCMN, PUNC, JSBR, NCMN, PUNC, NCMN, NEG, VSTA, PUNC, NCMN, JSBR, VSTA, PUNC, PNTR, NCMN, PUNC, NCMN, NCMN, PUNC, PUNC, PPRS, XVMM, VACT, RPRE, NCMN, NCMN, NCMN, PUNC, DDAC, VATT, ADVN, ADVN, ADVN, PUNC, NCMN, PUNC, PUNC, VSTA, XVBM, VSTA, JCMP, PUNC, FIXN, VACT, XVBM, VSTA, RPRE, PUNC, ...]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-p, I-p, I-p, I-p, I-p, I-p, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>[เลือก,  , ซัว, เร, ซ,  , อีก, เสียง,  , เพราะ, ซัว, เร, ซ,  , ช่วง, พีค,  , ถ้า, ได้มา, อยู่, ใน, ยุค,  , คล็อปป์,  , ยังไง, ก้อ, การันตี,  , 11,  , ตัวจริง, ครับ,  ]</td>\n",
       "      <td>[VACT, PUNC, NCMN, NCMN, NCMN, PUNC, DDBQ, NCMN, PUNC, JSBR, NCMN, NCMN, NCMN, PUNC, NCMN, NCMN, PUNC, JSBR, VSTA, XVAE, RPRE, NCMN, PUNC, NCMN, PUNC, VACT, NCMN, NCMN, PUNC, DCNM, PUNC, CNIT, RPRE, PUNC]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>275</td>\n",
       "      <td>[มุมมอง, ผม,  , ผม, ว่า, มัน, ไม่, ได้, สำคัญ, เท่าไร,  , ว่า, จะ, ได้, บรรจุ, ใน, กีฬา, รึ, ป่าว,  , ตราบใดที่, ยังมี, คน, สนใจ,  , มี, เงินหมุนเวียน,  , และ, ยัง, โต, ขึ้น, เรื่อย ๆ,  , เมื่อ, มัน, ใหญ่โต, จนถึง, จุด, ที่, เป็น, กระแส, หลัก,  , คำ, ว่า, จะ, เป็น, กีฬา, หรือไม่, เป็น,  , มัน, ก็, ไม่, มีความหมาย, แล้ว]</td>\n",
       "      <td>[NCMN, PPRS, VACT, PPRS, JSBR, PPRS, NEG, XVAM, VATT, PNTR, PUNC, JSBR, XVBM, XVAM, VACT, RPRE, NCMN, NCMN, NCMN, PUNC, JSBR, VACT, NCMN, VACT, NCMN, VSTA, NCMN, NCMN, JCRG, XVBM, VATT, XVAE, NCMN, NCMN, JSBR, PPRS, VATT, RPRE, NCMN, PREL, VSTA, NCMN, VATT, VACT, NCMN, JSBR, XVBM, VSTA, NCMN, EITT, VSTA, PUNC, PPRS, JSBR, NEG, VSTA, XVAE]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>399</td>\n",
       "      <td>[ออฟฟิศ, ดีกว่า, ครับ,  , เพราะ, ด้วย, ลักษณะ, งาน, ผม, ทำ, จาก, ที่, บ้าน, ไม่, ได้,  , แต่, ก็, มี, บางครั้ง, ที่, เอางาน, ใส่, แฟลช, ไดรฟ์, กลับมา, ทำ,  , เป็น, พวก, รายงาน, จุกจิก, ที่, ไม่ต้อง, เข้า, โปรแกรม, ภายใน,  , ซึ่ง, มัน, ก็, เล็กน้อย, เท่านั้น]</td>\n",
       "      <td>[NCMN, JSBR, NCMN, PUNC, JSBR, RPRE, NCMN, NCMN, NCMN, VACT, RPRE, PREL, NCMN, NEG, XVAE, PUNC, JCRG, JSBR, VSTA, NCMN, PREL, VACT, VACT, NCMN, NCMN, VACT, VACT, PUNC, VSTA, NCMN, NCMN, NCMN, PREL, XVMM, VACT, NCMN, RPRE, PUNC, JSBR, PPRS, JSBR, VATT, ADVN]</td>\n",
       "      <td>[B-c, I-c, I-c, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>268</td>\n",
       "      <td>[ใน, การ, ใส่, หูฟัง, ทำงาน, ผม, คิด, ว่า, ช่วย, นะ, ครับ,  , แต่, ก็, ไม่, ทุกคน, ที่, ใช้, วีธี, นี้, ได้ผล,  , บางคน, อาจจะ, ชอบ, ใส่, หูฟัง, เก็บเสียง, เฉย, ๆ,  , ไม่, ได้, ฟังเพลง,  , ขอ, แค่, ให้, เงียบ, ที่สุด,  , บางคน, เพราะ, ไม่, อยาก, ให้, เงียบ, เกินไป, มัน, วังเวง, พิกล,  , หรือ, ถ้า, ถอด, หูฟัง,  , เสียง, คน, คุย, กัน, ถึง, จะ, แค่, เบา, ๆ,  , เสียงเลื่อน, เก้าอี้,  , ของ, ตก, เล็ก, ๆ, น้อย, ๆ, ก็, ทำให้, เสีย, สมาธิ, แล้ว,  , แก้ปัญหา, ด้วย, การ, ฟังเพลง, ครับ,  , จังหวะ, ทำนอง, มัน, ไดรฟ์, ไป, ข้างหน้า,  , มัน, ก็, เหมือน, กระตุ้น, ให้, ...]</td>\n",
       "      <td>[RPRE, FIXN, VACT, NCMN, VACT, PPRS, VACT, JSBR, VACT, NCMN, NCMN, PUNC, JCRG, JSBR, NEG, VATT, PREL, VACT, NCMN, DDAC, VSTA, PUNC, NCMN, XVMM, VSTA, VACT, NCMN, NCMN, NCMN, PUNC, PUNC, NEG, XVAM, NCMN, PUNC, VACT, VACT, JSBR, VATT, ADVN, PUNC, VACT, JSBR, NEG, XVAM, JSBR, VSTA, ADVN, PPRS, VACT, NCMN, PUNC, JCRG, JSBR, VACT, NCMN, PUNC, NCMN, NCMN, VACT, ADVN, RPRE, XVBM, VACT, NCMN, PUNC, PUNC, NCMN, NCMN, PUNC, RPRE, NCMN, VATT, PUNC, VATT, PUNC, JSBR, VACT, VSTA, NCMN, XVAE, PUNC, VACT, RPRE, FIXN, VACT, NCMN, PUNC, NCMN, NCMN, PPRS, VACT, XVAE, NCMN, PUNC, PPRS, JSBR, VSTA, VACT, JSBR, ...]</td>\n",
       "      <td>[B-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, I-c, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, O, O, O, O, O, O, O, B-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, I-p, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer  \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ใส่', 'กระดาษ', 'จะ', 'ทำให้', 'จำได้', 'ง่าย', ' ', ' ', 'เพราะ', ' ', 'ทำ', 'ใน', 'คอม', ' ', 'มีระบบ', ' ', 'ใส่', 'คำสั่ง', 'ให้', 'เอง', ' ', 'ถึง', 'เวลา', 'สอบ', 'สมัครงาน', 'บาง', 'ที่', ' ', 'ให้', 'เขียน', 'ลง', 'กระดาษ', ' ', 'ก็', 'จะ', 'เขียน', 'ไม่', 'ออก', ' ']\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][2]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁', 'ใส่', '▁', 'กระดาษ', '▁จะ', '▁ทําให้', '▁', 'จําได้', '▁', 'ง่าย', '▁', '▁', '▁เพราะ', '▁', '▁ทํา', '▁', 'ใน', '▁', 'คอม', '▁', '▁', 'มีระบบ', '▁', '▁', 'ใส่', '▁', 'คําสั่ง', '▁', 'ให้', '▁', 'เอง', '▁', '▁', 'ถึง', '▁เวลา', '▁', 'สอบ', '▁', 'สมัครงาน', '▁บาง', '▁', 'ที่', '▁', '▁', 'ให้', '▁', 'เขียน', '▁', 'ลง', '▁', 'กระดาษ', '▁', '▁ก็', '▁จะ', '▁', 'เขียน', '▁ไม่', '▁', 'ออก', '▁', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 62\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 10, 10539, 10, 15486, 10, 6850, 10, 4738, 13764, 10, 1599, 10, 499, 10, 711, 10, 1599, 10, 662, 10, 711, 1739, 222, 6989, 10, 111, 10, 158, 10, 6]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.12ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.71ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 552\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'p': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, tokens, id. If pos_tags, ner_tags, tokens, id are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 138\n",
      "  Batch size = 12\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.6998698711395264,\n",
       " 'eval_precision': 0.4312080536912752,\n",
       " 'eval_recall': 0.6104513064133017,\n",
       " 'eval_f1': 0.5054080629301868,\n",
       " 'eval_accuracy': 0.6628451799421793,\n",
       " 'eval_runtime': 2.6994,\n",
       " 'eval_samples_per_second': 51.123,\n",
       " 'eval_steps_per_second': 4.445,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `CamembertForTokenClassification.forward` and have been ignored: pos_tags, ner_tags, tokens, id. If pos_tags, ner_tags, tokens, id are not expected by `CamembertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 138\n",
      "  Batch size = 12\n",
      " 92%|█████████▏| 11/12 [00:01<00:00,  5.48it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c': {'precision': 0.5176848874598071,\n",
       "  'recall': 0.6822033898305084,\n",
       "  'f1': 0.5886654478976234,\n",
       "  'number': 236},\n",
       " 'p': {'precision': 0.3368421052631579,\n",
       "  'recall': 0.518918918918919,\n",
       "  'f1': 0.4085106382978724,\n",
       "  'number': 185},\n",
       " 'overall_precision': 0.4312080536912752,\n",
       " 'overall_recall': 0.6104513064133017,\n",
       " 'overall_f1': 0.5054080629301868,\n",
       " 'overall_accuracy': 0.6628451799421793}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁สําหรับ', '▁เรา', '▁', 'จบ', '▁', 'โท', '▁', '▁', 'ที่', '▁', '▁', 'ออสเตรเลีย', '▁', '▁และ', '▁', 'นิวซีแลนด์', '▁', '▁เรา', '▁', 'ว่า', '▁', 'คุ้ม', '▁', 'ยิ่งกว่า', '▁', 'คุ้ม', '▁', '▁', 'เปลี่ยน', '▁ชีวิต', '▁', '▁', 'มุมมอง', '▁', '▁', 'ทัศนคติ', '▁', '▁', 'คอนเน', 'ค', '▁', 'ชั่น', '▁', '▁งาน', '▁', '▁', 'เงิน', '▁แน่นอน', '▁', 'ว่า', '▁', 'ใช้เวลา', '▁', 'ค่ะ', '▁', '▁แต่', '▁ถ้า', '▁', 'มีโอกาส', '▁', 'ไป', '▁', 'เถอะ', '▁', 'ค่ะ', '▁', '▁คน', '▁มี', '▁', 'ความสามารถ', '▁', 'มักจะ', '▁มี', '▁', 'หนทาง', '▁', 'ต่อไป', '▁', 'เรื่อยๆ', '</s>']\n"
     ]
    }
   ],
   "source": [
    "idx = 20\n",
    "show_text = tokenizer(tokenized_datasets[\"test\"][idx]['tokens'], is_split_into_words=True)\n",
    "print(tokenizer.convert_ids_to_tokens(show_text['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: ['B-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-c', 'I-c', 'I-c', 'I-p', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p']\n",
      "true: ['B-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'I-c', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p']\n"
     ]
    }
   ],
   "source": [
    "print('pred:', true_predictions[idx])\n",
    "print('true:', true_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ฉัน', 'ชอบ', 'หมา', 'เพราะ', 'มัน', 'น่ารัก', 'มาก', 'ๆ', ' ', 'เลย']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "text_list = word_tokenize('ฉันชอบหมาเพราะมันน่ารักมากๆ เลย')\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 264, 1879, 10, 1022, 474, 661, 5840, 10, 82, 10, 34, 10, 10, 48, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁ฉัน', '▁ชอบ', '▁', 'หมา', '▁เพราะ', '▁มัน', '▁น่ารัก', '▁', 'มาก', '▁', 'ๆ', '▁', '▁', 'เลย', '</s>']\n"
     ]
    }
   ],
   "source": [
    "_input_token = tokenizer(text_list, is_split_into_words=True)\n",
    "_word_token = tokenizer.convert_ids_to_tokens(_input_token[\"input_ids\"])\n",
    "print(_input_token)\n",
    "print(_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 12\n",
      "13it [00:36,  8.10s/it]                        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4, 1, 2, 2, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trainer.predict([_input_token])[0]\n",
    "pred = np.argmax(pred, axis=2)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-c', 'I-c', 'I-c', 'I-c', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p']\n"
     ]
    }
   ],
   "source": [
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(pred, labels)\n",
    "][0]\n",
    "print(true_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(true_predictions))\n",
    "print(len(_word_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('▁ฉัน', 'B-c')\n",
      "('▁ชอบ', 'I-c')\n",
      "('▁', 'I-c')\n",
      "('หมา', 'I-c')\n",
      "('▁เพราะ', 'B-p')\n",
      "('▁มัน', 'I-p')\n",
      "('▁น่ารัก', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('มาก', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('ๆ', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('▁', 'I-p')\n",
      "('เลย', 'I-p')\n"
     ]
    }
   ],
   "source": [
    "for w, l in zip(_word_token[1:-1], true_predictions):\n",
    "  print((w, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018e0a3ac4678c6eee4f5b6012f6866bd583f46fe819b31cdc8524b9233bdcf3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wangchan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
