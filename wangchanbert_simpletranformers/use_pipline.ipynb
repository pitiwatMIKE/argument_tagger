{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\pitiw/.cache\\huggingface\\transformers\\850f85e628f0d9ff7857833e8542c87ce4f4e4e554845aec2c2f0f9b798e9602.55fbe44df3c0d4d4b0f176ea9200b724f4cf138bb7ec54dc30097525d7ae4cad\n",
      "loading file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/special_tokens_map.json from cache at C:\\Users\\pitiw/.cache\\huggingface\\transformers\\7e62dc9b3b2ba4831acaeb43f8eabd2ca63808df54422ea756a9dd0f2d5d514d.f205401a3c03892a215cbf4ca4f121cde3dc5b29139e8df3e4d91e1e82096ed5\n",
      "loading file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/tokenizer_config.json from cache at C:\\Users\\pitiw/.cache\\huggingface\\transformers\\da6407eb6ca4836ed3292733002ea5dc3f0603d8523f1b38bece0778fa0b911a.20ec7b64e0a3a93a2b62f2ee7806acaa13e532a159e2ca4d2f0fae49abe2228a\n",
      "loading configuration file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/config.json from cache at C:\\Users\\pitiw/.cache\\huggingface\\transformers\\62eaa274535104614d51ce5bdd27bd7d0b1f855de3d5c4ef42e5a24bfe9c3267.c25da08a9d3f00d38c81507951ba2e22f808340332cde6c6f1b54bf41ad3ca93\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"pitiwat/argument_wangchanberta2\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-CLAIM\",\n",
      "    \"2\": \"B-PREMIST\",\n",
      "    \"3\": \"I-CLAIM\",\n",
      "    \"4\": \"I-PREMIST\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pitiwat/argument_wangchanberta2/resolve/main/pytorch_model.bin from cache at C:\\Users\\pitiw/.cache\\huggingface\\transformers\\3dde44595f4fbecb3b19b7368dfaf684ca80815b7e342abe95b8f52f152d552e.bdd43a288d395b673b2c0ba3416530504a89cb848b2a76e3b8c6ce89c1b5e079\n",
      "All model checkpoint weights were used when initializing CamembertForTokenClassification.\n",
      "\n",
      "All the weights of CamembertForTokenClassification were initialized from the model checkpoint at pitiwat/argument_wangchanberta2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CamembertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline, TokenClassificationPipeline, Trainer\n",
    "\n",
    "model_path = 'pitiwat/argument_wangchanberta2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,  model_max_length=512)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# pipe = pipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ฉัน ชอบ หมา \\n เพราะ มัน น่ารัก'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "text = '''ฉันชอบหมา\n",
    "เพราะมันน่ารัก'''\n",
    "\n",
    "text_token = word_tokenize(text)\n",
    "text_token = ' '.join(text_token)\n",
    "text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'CLAIM', 'score': 0.99965537, 'word': 'ฉัน ชอบ หมา', 'start': 0, 'end': 11}\n",
      "{'entity_group': 'PREMIST', 'score': 0.9981974, 'word': 'เพราะ มัน น่ารัก', 'start': 13, 'end': 30}\n"
     ]
    }
   ],
   "source": [
    "prediction = pipe(text_token, grouped_entities=True, ignore_labels=[])\n",
    "for dict_pred in prediction:\n",
    "    print(dict_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ฉันชอบ', 'หมา', 'เพราะมัน', 'น่ารัก']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('ฉันชอบหมาเพราะมันน่ารัก').tokens()[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '▁ฉันชอบ', '▁', 'หมา', '▁', 'เพราะมัน', '▁น่ารัก', '</s>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(tokenizer('ฉันชอบหมาเพราะมันน่ารัก').tokens()[1:-1], is_split_into_words=True).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018e0a3ac4678c6eee4f5b6012f6866bd583f46fe819b31cdc8524b9233bdcf3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wangchan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
