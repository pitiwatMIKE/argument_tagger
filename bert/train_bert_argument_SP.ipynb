{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n",
      "[('ถ้า', 'B-c'), ('เดินทาง', 'I-c'), ('กลางคืน', 'I-c'), ('ก็', 'I-c'), ('รถทัวร์', 'I-c'), ('ครับ', 'I-c'), ('[SP]', 'I-c'), ('[SP]', 'O'), ('เพราะ', 'B-p'), ('รถ', 'I-p'), ('ไม่', 'I-p'), ('เยอะ', 'I-p'), ('[SP]', 'I-p'), ('ความเสี่ยง', 'I-p'), ('การ', 'I-p'), ('เกิด', 'I-p'), ('อุบัติ', 'I-p'), ('ห', 'I-p'), ('ตุ', 'I-p'), ('ก็', 'I-p'), ('น้อย', 'I-p'), ('(', 'I-p'), ('มั้ง', 'I-p'), (')', 'I-p'), ('[SP]', 'I-p'), ('[SP]', 'O'), ('ถ้า', 'B-c'), ('กลางวัน', 'I-c'), ('ก็', 'I-c'), ('เครื่องบิน', 'I-c'), ('ครับ', 'I-c'), ('[SP]', 'I-c'), ('[SP]', 'O'), ('เพราะ', 'B-p'), ('[SP]', 'I-p'), ('มัน', 'I-p'), ('ใช้เวลา', 'I-p'), ('น้อย', 'I-p'), ('จะ', 'I-p'), ('ได้', 'I-p'), ('มี', 'I-p'), ('เวลา', 'I-p'), ('ระหว่าง', 'I-p'), ('วัน', 'I-p'), ('เยอะ', 'I-p'), ('ๆ', 'I-p'), ('[SP]', 'I-p')]\n"
     ]
    }
   ],
   "source": [
    "path_name = \"../dataset/data/\"\n",
    "\n",
    "with open(path_name + 'comment-pos.data', 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile:\n",
    "    text_inside = []\n",
    "    for word, pos, label in data:\n",
    "        if word.strip() == '':\n",
    "            text_inside.append(('[SP]', label))\n",
    "        else:\n",
    "            text_inside.append((word, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NER_TAGS = [\n",
    "        \"O\",\n",
    "        \"B_C\",\n",
    "        \"B_P\",\n",
    "        \"I_C\",\n",
    "        \"I_P\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_simple_transformer_format(sentences):\n",
    "    sentence_id = []\n",
    "    words = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, sents in enumerate(sentences):\n",
    "        for word, label in sents:\n",
    "            label = label.upper().replace(\"-\", \"_\")\n",
    "            sentence_id.append(idx)\n",
    "            words.append(word)\n",
    "            labels.append(label)\n",
    "    return pd.DataFrame(\n",
    "        {\"sentence_id\": sentence_id, \"words\": words, \"labels\": labels}\n",
    "    )    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>อะไหล่</td>\n",
       "      <td>B_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>เทอร์โบ</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[SP]</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>อี</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ซุ</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37405</th>\n",
       "      <td>551</td>\n",
       "      <td>ทำ</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37406</th>\n",
       "      <td>551</td>\n",
       "      <td>อะไร</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37407</th>\n",
       "      <td>551</td>\n",
       "      <td>ได้</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37408</th>\n",
       "      <td>551</td>\n",
       "      <td>หลายอย่าง</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37409</th>\n",
       "      <td>551</td>\n",
       "      <td>[SP]</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id      words labels\n",
       "0                0     อะไหล่    B_C\n",
       "1                0    เทอร์โบ    I_C\n",
       "2                0       [SP]    I_C\n",
       "3                0         อี    I_C\n",
       "4                0         ซุ    I_C\n",
       "...            ...        ...    ...\n",
       "37405          551         ทำ    I_P\n",
       "37406          551       อะไร    I_P\n",
       "37407          551        ได้    I_P\n",
       "37408          551  หลายอย่าง    I_P\n",
       "37409          551       [SP]    I_P\n",
       "\n",
       "[37410 rows x 3 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = convert_to_simple_transformer_format(train_sents)\n",
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = convert_to_simple_transformer_format(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monsoon-nlp/bert-base-thai were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monsoon-nlp/bert-base-thai and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "\n",
    "# Configure the model\n",
    "ner_args = NERArgs()\n",
    "ner_args.train_batch_size = 12\n",
    "ner_args.evaluate_during_training = False\n",
    "ner_args.overwrite_output_dir = True\n",
    "ner_args.num_train_epochs = 100 #10\n",
    "\n",
    "\n",
    "model = NERModel(\n",
    "    \"bert\", \"monsoon-nlp/bert-base-thai\", args=ner_args, use_cuda=torch.cuda.is_available(), labels=_NER_TAGS\n",
    ")\n",
    "\n",
    "# Train the modelk\n",
    "model.train_model(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# result, model_outputs, preds_list = test_ner.eval_model(test_)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ผม เห็น ปัญหา นี้ เยอะ มา ก. .. พ่อแม่ [SP] สั่งสอน ลูก ไม่ ได้ [SP] ยิ่ง เด็ก ตัวเล็ก [SP] ๆ [SP] พ่อแม่ [SP] โยน มือถือ ให้ เพราะ ไม่ อยาก ให้ รบกวน ตัวเอง [SP] พอให้ เรียน ออนไลน์ บอก ไม่ มี อุปกรณ์ [SP] ปล. จาก ที่ ผม เห็น มา จริง ๆ [SP] นะ ครับ\n",
      "[('ผม', 'B-c'), ('เห็น', 'I-c'), ('ปัญหา', 'I-c'), ('นี้', 'I-c'), ('เยอะ', 'I-c'), ('มา', 'I-c'), ('ก.', 'I-c'), ('..', 'I-c'), ('พ่อแม่', 'I-c'), ('[SP]', 'I-c'), ('สั่งสอน', 'I-c'), ('ลูก', 'I-c'), ('ไม่', 'I-c'), ('ได้', 'I-c'), ('[SP]', 'I-c'), ('ยิ่ง', 'I-c'), ('เด็ก', 'I-c'), ('ตัวเล็ก', 'I-c'), ('[SP]', 'I-c'), ('ๆ', 'I-c'), ('[SP]', 'I-c'), ('พ่อแม่', 'I-c'), ('[SP]', 'I-c'), ('โยน', 'I-c'), ('มือถือ', 'I-c'), ('ให้', 'I-c'), ('เพราะ', 'B-p'), ('ไม่', 'I-p'), ('อยาก', 'I-p'), ('ให้', 'I-p'), ('รบกวน', 'I-p'), ('ตัวเอง', 'I-p'), ('[SP]', 'I-p'), ('พอให้', 'I-p'), ('เรียน', 'I-p'), ('ออนไลน์', 'I-p'), ('บอก', 'I-p'), ('ไม่', 'I-p'), ('มี', 'I-p'), ('อุปกรณ์', 'I-p'), ('[SP]', 'O'), ('ปล.', 'O'), ('จาก', 'O'), ('ที่', 'O'), ('ผม', 'O'), ('เห็น', 'O'), ('มา', 'O'), ('จริง ๆ', 'O'), ('[SP]', 'O'), ('นะ', 'O'), ('ครับ', 'O')]\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "test_pred = \" \".join(list(map(lambda word: word[0], test_sents[idx])))\n",
    "print(test_pred)\n",
    "print(test_sents[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.30s/it]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 31.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'หาก': 'B_C'}, {'พิจารณา': 'B_C'}, {'จาก': 'B_C'}, {'การเข้าสู่': 'I_P'}, {'การ': 'B_C'}, {'เป็น': 'B_C'}, {'[SP]': 'B_C'}, {'AEC': 'B_C'}, {'[SP]': 'B_C'}, {'ใน': 'B_C'}, {'สิ้นปี': 'B_C'}, {'นี้': 'B_C'}, {'แล้ว': 'B_C'}, {'[SP]': 'O'}, {'ผม': 'I_P'}, {'ว่า': 'B_P'}, {'ความสามารถ': 'B_C'}, {'ทาง': 'B_C'}, {'ด้าน': 'B_C'}, {'ภาษาอังกฤษ': 'I_P'}, {'จะ': 'B_C'}, {'ใช้ประโยชน์': 'B_C'}, {'ได้': 'B_C'}, {'มากกว่า': 'B_C'}, {'คณิตศาสตร์': 'B_C'}, {'เยอะ': 'B_C'}, {'มาก': 'B_C'}, {'ครับ': 'B_C'}, {'[SP]': 'B_C'}, {'ได้': 'B_C'}, {'ทั้ง': 'B_C'}, {'ด้าน': 'B_C'}, {'ติดต่อ': 'B_C'}, {'ธุรกิจ': 'I_P'}, {'[SP]': 'B_C'}, {'การท่องเที่ยว': 'I_P'}, {'[SP]': 'B_C'}, {'การศึกษา': 'B_C'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "predictions, raw_outputs = model.predict([test_pred])\n",
    "print(predictions[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.24s/it]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ผม': 'B_C'}, {'เห็น': 'B_C'}, {'ปัญหา': 'B_C'}, {'นี้': 'I_P'}, {'เยอะ': 'B_C'}, {'มา': 'B_C'}, {'ก.': 'B_C'}, {'..': 'B_C'}, {'พ่อแม่': 'B_C'}, {'[SP]': 'B_C'}, {'สั่งสอน': 'B_C'}, {'ลูก': 'B_C'}, {'ไม่': 'B_C'}, {'ได้': 'O'}, {'[SP]': 'B_C'}, {'ยิ่ง': 'B_P'}, {'เด็ก': 'B_C'}, {'ตัวเล็ก': 'B_C'}, {'[SP]': 'B_C'}, {'ๆ': 'B_P'}, {'[SP]': 'B_C'}, {'พ่อแม่': 'B_C'}, {'[SP]': 'B_C'}, {'โยน': 'B_C'}, {'มือถือ': 'B_C'}, {'ให้': 'B_C'}, {'เพราะ': 'B_C'}, {'ไม่': 'B_C'}, {'อยาก': 'B_C'}, {'ให้': 'B_C'}, {'รบกวน': 'B_C'}, {'ตัวเอง': 'B_C'}, {'[SP]': 'B_C'}, {'พอให้': 'I_P'}, {'เรียน': 'B_C'}, {'ออนไลน์': 'B_C'}, {'บอก': 'B_C'}, {'ไม่': 'B_C'}, {'มี': 'B_C'}, {'อุปกรณ์': 'B_C'}, {'[SP]': 'B_C'}, {'ปล.': 'B_C'}, {'จาก': 'B_C'}, {'ที่': 'B_C'}, {'ผม': 'I_P'}, {'เห็น': 'B_C'}, {'มา': 'B_C'}, {'จริง ๆ': 'B_C'}, {'[SP]': 'B_C'}, {'นะ': 'B_C'}, {'ครับ': 'I_P'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_test = (list(map(lambda word: word[0], test_sents[10])))\n",
    "predictions, raw_outputs = model.predict([_test], split_on_space=False)\n",
    "print(predictions[0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monsoon-nlp/bert-base-thai were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monsoon-nlp/bert-base-thai and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "ner_args = NERArgs()\n",
    "model = NERModel(\n",
    "    \"bert\", \"monsoon-nlp/bert-base-thai\", args=ner_args, use_cuda=torch.cuda.is_available(), labels=_NER_TAGS\n",
    ")\n",
    "\n",
    "\n",
    "test_ner = NERModel(\"bert\", './weight3/checkpoint-4600-epoch-100', args=ner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'B_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "for sent in test_sents:\n",
    "    labels = []\n",
    "    for word, label in sent:\n",
    "        label = label.upper().replace(\"-\", \"_\")\n",
    "        labels.append(label)\n",
    "    y_test.append(labels)\n",
    "    \n",
    "print(y_test[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.66s/it]\n",
      "Running Prediction: 100%|██████████| 18/18 [00:02<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'O', 'O', 'O', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'I_P', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'I_P', 'O', 'I_P', 'O', 'O', 'O', 'O', 'O', 'O', 'I_P', 'O', 'I_P', 'O', 'O', 'O', 'I_P', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I_C', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_P', 'I_P', 'I_P', 'O', 'B_P', 'I_P', 'I_P', 'I_P', 'O', 'I_P', 'O', 'O', 'I_P', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P']\n"
     ]
    }
   ],
   "source": [
    "test_list = []\n",
    "for sent in test_sents:\n",
    "    words = []\n",
    "    for word, label in sent:\n",
    "        words.append(word)\n",
    "    test_list.append(\" \".join(words))\n",
    "\n",
    "predictions, raw_outputs = test_ner.predict(test_list)\n",
    "\n",
    "y_pred = []\n",
    "for preds in predictions:\n",
    "    y_pred.append([list(pred.items())[0][1] for pred in preds])\n",
    "\n",
    "print(y_pred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\project-argument-tagger\\bert\\train_bert_argument_SP.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/train_bert_argument_SP.ipynb#ch0000032?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_sents[idx_])):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/train_bert_argument_SP.ipynb#ch0000032?line=3'>4</a>\u001b[0m     word_test \u001b[39m=\u001b[39m test_sents[idx_][i][\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/train_bert_argument_SP.ipynb#ch0000032?line=4'>5</a>\u001b[0m     word_pred \u001b[39m=\u001b[39m predictions_[idx_][i][\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/train_bert_argument_SP.ipynb#ch0000032?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m word_test \u001b[39m!=\u001b[39m word_pred:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/train_bert_argument_SP.ipynb#ch0000032?line=6'>7</a>\u001b[0m         \u001b[39mprint\u001b[39m(i, word_test, \u001b[39m'\u001b[39m\u001b[39m||\u001b[39m\u001b[39m'\u001b[39m, word_pred)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "idx_ = 115 \n",
    "temp = []\n",
    "for i in range(len(test_sents[idx_])):\n",
    "    word_test = test_sents[idx_][i][0]\n",
    "    word_pred = predictions_[idx_][i][0]\n",
    "    if word_test != word_pred:\n",
    "        print(i, word_test, '||', word_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('เห็นด้วย', 'B-c'), ('กับ', 'I-c'), ('[SP]', 'I-c'), ('คห.', 'I-c'), ('[SP]', 'I-c'), ('บน', 'I-c'), ('[SP]', 'I-c'), ('ว่า', 'I-c'), ('ไม่', 'I-c'), ('ใช่', 'I-c'), ('การ', 'I-c'), ('สอบ', 'I-c'), ('ได้', 'I-c'), ('เพราะ', 'B-p'), ('ใคร', 'I-p'), ('ไป', 'I-p'), ('สอบ', 'I-p'), ('ส่วนใหญ่', 'I-p'), ('ก็ได้', 'I-p'), ('มัน', 'I-p'), ('เหมือน', 'I-p'), ('พ่อแม่', 'I-p'), ('จ่าย', 'I-p'), ('เงิน', 'I-p'), ('ให้', 'I-p'), ('ลูก', 'I-p'), ('ไป', 'I-p'), ('เรียน', 'I-p'), ('[SP]', 'I-p'), ('highschool', 'I-p'), ('[SP]', 'I-p'), ('ที่', 'I-p'), ('[SP]', 'I-p'), ('ตปท.', 'I-p'), ('[SP]', 'I-p'), ('1', 'I-p'), ('[SP]', 'I-p'), ('ปี', 'I-p'), ('[SP]', 'O'), ('ส่วนตัว', 'O'), ('เคย', 'O'), ('เป็น', 'O'), ('[SP]', 'O'), ('นร.', 'O'), ('[SP]', 'O'), ('แลกเปลี่ยน', 'O'), ('เมื่อ', 'O'), ('[SP]', 'O'), ('20', 'O'), ('ปี', 'O'), ('ที่แล้ว', 'O'), ('[SP]', 'O'), ('เป็น', 'O'), ('ประสบการณ์', 'O'), ('ที่', 'O'), ('ดีมาก', 'O'), ('ๆ', 'O'), ('นอกจาก', 'O'), ('ได้', 'O'), ('ภาษา', 'O'), ('[SP]', 'O'), ('ยัง', 'O'), ('ได้', 'O'), ('เรียนรู้', 'O'), ('วัฒนธรรม', 'O'), ('[SP]', 'O'), ('ได้', 'O'), ('อยู่', 'O'), ('กับ', 'O'), ('family', 'O'), ('[SP]', 'O'), ('ของ', 'O'), ('เค้า', 'O'), ('จริงๆ', 'O'), ('[SP]', 'O'), ('ถ้า', 'O'), ('เด็ก', 'O'), ('คนอื่นๆ', 'O'), ('ไป', 'O'), ('เอง', 'O'), ('อยู่', 'O'), ('หอ', 'O'), ('ก็', 'O'), ('จะ', 'O'), ('ไม่', 'O'), ('ได้', 'O'), ('ประสบการณ์', 'O'), ('แบบนี้', 'O'), ('[SP]', 'O'), ('คห', 'O'), ('สต.', 'O'), ('[SP]', 'O'), ('ถ้า', 'O'), ('มี', 'O'), ('ทุนทรัพย์', 'O'), ('เพียงพอ', 'O'), ('ก็', 'O'), ('น่า', 'O'), ('ไป', 'O'), ('ค่ะ', 'O')]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(test_sents[idx_])\n",
    "print(len(test_sents[idx_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('เห็นด้วย', 'B_C'), ('กับ', 'I_C'), ('[SP]', 'O'), ('คห.', 'I_C'), ('[SP]', 'O'), ('บน', 'I_C'), ('[SP]', 'I_P'), ('ว่า', 'I_C'), ('ไม่', 'I_C'), ('ใช่', 'I_C'), ('การ', 'I_P'), ('สอบ', 'I_C'), ('ได้', 'I_P'), ('เพราะ', 'B_P'), ('ใคร', 'I_P'), ('ไป', 'I_P'), ('สอบ', 'I_P'), ('ส่วนใหญ่', 'I_P'), ('ก็ได้', 'I_P'), ('มัน', 'I_P'), ('เหมือน', 'I_P'), ('พ่อแม่', 'I_P'), ('จ่าย', 'I_P'), ('เงิน', 'I_P'), ('ให้', 'I_P'), ('ลูก', 'I_P'), ('ไป', 'I_P'), ('เรียน', 'I_P'), ('[SP]', 'I_P'), ('highschool', 'I_P'), ('[SP]', 'I_P'), ('ที่', 'I_P'), ('[SP]', 'I_P'), ('ตปท.', 'I_P'), ('[SP]', 'I_P'), ('1', 'I_P'), ('[SP]', 'I_P'), ('ปี', 'I_P'), ('[SP]', 'O'), ('ส่วนตัว', 'B_C'), ('เคย', 'O'), ('เป็น', 'O'), ('[SP]', 'O'), ('นร.', 'O'), ('[SP]', 'O'), ('แลกเปลี่ยน', 'O'), ('เมื่อ', 'O'), ('[SP]', 'O'), ('20', 'O'), ('ปี', 'O'), ('ที่แล้ว', 'O'), ('[SP]', 'O'), ('เป็น', 'O'), ('ประสบการณ์', 'O'), ('ที่', 'O'), ('ดีมาก', 'O'), ('ๆ', 'O'), ('นอกจาก', 'O'), ('ได้', 'O'), ('ภาษา', 'O'), ('[SP]', 'O'), ('ยัง', 'O'), ('ได้', 'O'), ('เรียนรู้', 'O'), ('วัฒนธรรม', 'O'), ('[SP]', 'O'), ('ได้', 'O'), ('อยู่', 'O'), ('กับ', 'O'), ('family', 'O'), ('[SP]', 'O'), ('ของ', 'O'), ('เค้า', 'I_C'), ('จริงๆ', 'O'), ('[SP]', 'O'), ('ถ้า', 'O'), ('เด็ก', 'I_C'), ('คนอื่นๆ', 'I_C'), ('ไป', 'O'), ('เอง', 'I_C'), ('อยู่', 'O'), ('หอ', 'I_C'), ('ก็', 'I_C'), ('จะ', 'O'), ('ไม่', 'O'), ('ได้', 'O'), ('ประสบการณ์', 'O'), ('แบบนี้', 'O'), ('[SP]', 'O')]\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "predictions_ = []\n",
    "for sent in predictions:\n",
    "    sent_ = []\n",
    "    for word_lable in sent:\n",
    "        sent_.append(list(word_lable.items())[0])\n",
    "    predictions_.append(sent_)\n",
    "\n",
    "print(predictions_[idx_])\n",
    "print(len(predictions_[idx_]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ปัญหา\n",
    "#### ติด ๆ แยกจากกันตรง index ที่ 10\n",
    "#### ติด เวลา predict มีการตัดข้อความทำให้ ข้อความ predict ไม่เท่ากับ test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End test ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6 8 10 18 19 21 23 29 34 36 37 39 42 44 55 60 62 71 81 85 97 98 99 101 106 115 120 121 123 124 127 128 133 134 "
     ]
    }
   ],
   "source": [
    "y_pred_ = []\n",
    "y_test_ = []\n",
    "for i in range(len(y_test)):\n",
    "    if len(y_pred[i]) != len(y_test[i]):\n",
    "        # print(len(y_pred[i]), len(y_test[i]))\n",
    "        print(i, end=\" \")\n",
    "        continue;\n",
    "    y_pred_.append(y_pred[i])\n",
    "    y_test_.append(y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 99\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred_), len(y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7135542980328753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          _C       0.24      0.37      0.29       111\n",
      "          _P       0.16      0.41      0.23       111\n",
      "\n",
      "   micro avg       0.19      0.39      0.26       222\n",
      "   macro avg       0.20      0.39      0.26       222\n",
      "weighted avg       0.20      0.39      0.26       222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "print(\"accuracy:\" ,accuracy_score(y_test_, y_pred_))\n",
    "print(classification_report(y_test_, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_continue = NERModel(\"bert\", './weigth_SP', args=ner_args, use_cuda=torch.cuda.is_available(), labels=_NER_TAGS)\n",
    "# model_continue.train_model(train_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6dc3a5d5e5e7987cc7d18355d6f408e2f42f41b0aca7d07c9009e9d98d8dd16"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('argument')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
