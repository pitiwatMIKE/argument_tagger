{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n",
      "[('ถ้า', 'B-c'), ('เดินทาง', 'I-c'), ('กลางคืน', 'I-c'), ('ก็', 'I-c'), ('รถทัวร์', 'I-c'), ('ครับ', 'I-c'), (' ', 'I-c'), (' ', 'O'), ('เพราะ', 'B-p'), ('รถ', 'I-p'), ('ไม่', 'I-p'), ('เยอะ', 'I-p'), (' ', 'I-p'), ('ความเสี่ยง', 'I-p'), ('การ', 'I-p'), ('เกิด', 'I-p'), ('อุบัติ', 'I-p'), ('ห', 'I-p'), ('ตุ', 'I-p'), ('ก็', 'I-p'), ('น้อย', 'I-p'), ('(', 'I-p'), ('มั้ง', 'I-p'), (')', 'I-p'), (' ', 'I-p'), (' ', 'O'), ('ถ้า', 'B-c'), ('กลางวัน', 'I-c'), ('ก็', 'I-c'), ('เครื่องบิน', 'I-c'), ('ครับ', 'I-c'), (' ', 'I-c'), (' ', 'O'), ('เพราะ', 'B-p'), (' ', 'I-p'), ('มัน', 'I-p'), ('ใช้เวลา', 'I-p'), ('น้อย', 'I-p'), ('จะ', 'I-p'), ('ได้', 'I-p'), ('มี', 'I-p'), ('เวลา', 'I-p'), ('ระหว่าง', 'I-p'), ('วัน', 'I-p'), ('เยอะ', 'I-p'), ('ๆ', 'I-p'), (' ', 'I-p')]\n"
     ]
    }
   ],
   "source": [
    "path_name = \"../dataset/data/\"\n",
    "\n",
    "with open(path_name + 'comment-pos.data', 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile:\n",
    "    text_inside = []\n",
    "    for word, pos, label in data:\n",
    "        text_inside.append((word, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NER_TAGS = [\n",
    "        \"O\",\n",
    "        \"B_C\",\n",
    "        \"B_P\",\n",
    "        \"I_C\",\n",
    "        \"I_P\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_simple_transformer_format(sentences):\n",
    "    sentence_id = []\n",
    "    words = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, sents in enumerate(sentences):\n",
    "        for word, label in sents:\n",
    "            label = label.upper().replace(\"-\", \"_\")\n",
    "            sentence_id.append(idx)\n",
    "            words.append(word)\n",
    "            labels.append(label)\n",
    "    return pd.DataFrame(\n",
    "        {\"sentence_id\": sentence_id, \"words\": words, \"labels\": labels}\n",
    "    )    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>อะไหล่</td>\n",
       "      <td>B_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>เทอร์โบ</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>อี</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ซุ</td>\n",
       "      <td>I_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37405</th>\n",
       "      <td>551</td>\n",
       "      <td>ทำ</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37406</th>\n",
       "      <td>551</td>\n",
       "      <td>อะไร</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37407</th>\n",
       "      <td>551</td>\n",
       "      <td>ได้</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37408</th>\n",
       "      <td>551</td>\n",
       "      <td>หลายอย่าง</td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37409</th>\n",
       "      <td>551</td>\n",
       "      <td></td>\n",
       "      <td>I_P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id      words labels\n",
       "0                0     อะไหล่    B_C\n",
       "1                0    เทอร์โบ    I_C\n",
       "2                0               I_C\n",
       "3                0         อี    I_C\n",
       "4                0         ซุ    I_C\n",
       "...            ...        ...    ...\n",
       "37405          551         ทำ    I_P\n",
       "37406          551       อะไร    I_P\n",
       "37407          551        ได้    I_P\n",
       "37408          551  หลายอย่าง    I_P\n",
       "37409          551               I_P\n",
       "\n",
       "[37410 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = convert_to_simple_transformer_format(train_sents)\n",
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = convert_to_simple_transformer_format(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type camembert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing BertForTokenClassification: ['roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'lm_head.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'classifier.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.attention.self.query.weight', 'classifier.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\project-argument-tagger\\bert\\user_bert_argument copy.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/user_bert_argument%20copy.ipynb#ch0000006?line=7'>8</a>\u001b[0m ner_args\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/user_bert_argument%20copy.ipynb#ch0000006?line=8'>9</a>\u001b[0m ner_args\u001b[39m.\u001b[39mnum_train_epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m#10\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/user_bert_argument%20copy.ipynb#ch0000006?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m NERModel(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/user_bert_argument%20copy.ipynb#ch0000006?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mbert\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mairesearch/wangchanberta-base-att-spm-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m, args\u001b[39m=\u001b[39;49mner_args, use_cuda\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available(), labels\u001b[39m=\u001b[39;49m_NER_TAGS\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/project-argument-tagger/bert/user_bert_argument%20copy.ipynb#ch0000006?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\argument\\lib\\site-packages\\simpletransformers\\ner\\ner_model.py:362\u001b[0m, in \u001b[0;36mNERModel.__init__\u001b[1;34m(self, model_type, model_name, labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=354'>355</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=355'>356</a>\u001b[0m         model_name,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=356'>357</a>\u001b[0m         do_lower_case\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdo_lower_case,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=357'>358</a>\u001b[0m         normalization\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=358'>359</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=359'>360</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=360'>361</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=361'>362</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=362'>363</a>\u001b[0m         model_name, do_lower_case\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdo_lower_case, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=363'>364</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=365'>366</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mspecial_tokens_list:\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=366'>367</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39madd_tokens(\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=367'>368</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mspecial_tokens_list, special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/simpletransformers/ner/ner_model.py?line=368'>369</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\argument\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1784'>1785</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1785'>1786</a>\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1787'>1788</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1788'>1789</a>\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1789'>1790</a>\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1790'>1791</a>\u001b[0m     init_configuration,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1791'>1792</a>\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1792'>1793</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1793'>1794</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1794'>1795</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1795'>1796</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\argument\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1923\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1920'>1921</a>\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1921'>1922</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1922'>1923</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1923'>1924</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1924'>1925</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1925'>1926</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1926'>1927</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/tokenization_utils_base.py?line=1927'>1928</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\argument\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:193\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=163'>164</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=164'>165</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=165'>166</a>\u001b[0m     vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=176'>177</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=177'>178</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=178'>179</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=179'>180</a>\u001b[0m         do_lower_case\u001b[39m=\u001b[39mdo_lower_case,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=180'>181</a>\u001b[0m         do_basic_tokenize\u001b[39m=\u001b[39mdo_basic_tokenize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=189'>190</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=190'>191</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=192'>193</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(vocab_file):\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=193'>194</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=194'>195</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a vocabulary file at path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mvocab_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. To load the vocabulary from a Google pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=195'>196</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmodel use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=196'>197</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\argument\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/genericpath.py?line=27'>28</a>\u001b[0m \u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/genericpath.py?line=28'>29</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/genericpath.py?line=29'>30</a>\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/genericpath.py?line=30'>31</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     <a href='file:///c%3A/Users/pitiw/miniconda3/envs/argument/lib/genericpath.py?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "\n",
    "# Configure the model\n",
    "ner_args = NERArgs()\n",
    "ner_args.train_batch_size = 12\n",
    "ner_args.evaluate_during_training = False\n",
    "ner_args.overwrite_output_dir = True\n",
    "ner_args.num_train_epochs = 100 #10\n",
    "\n",
    "\n",
    "model = NERModel(\n",
    "    \"bert\", \"airesearch/wangchanberta-base-att-spm-uncased\", args=ner_args, use_cuda=torch.cuda.is_available(), labels=_NER_TAGS\n",
    ")\n",
    "\n",
    "# Train the modelk\n",
    "# model.train_model(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ner = NERModel(\"bert\", 'weight1/checkpoint-4600-epoch-100', args=ner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.86s/it]\n",
      "Running Evaluation: 100%|██████████| 18/18 [00:04<00:00,  4.38it/s]\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\pitiw\\miniconda3\\envs\\argument\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.813645319806205,\n",
       " 'precision': 0.08484848484848485,\n",
       " 'recall': 0.27450980392156865,\n",
       " 'f1_score': 0.12962962962962965}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, preds_list = test_ner.eval_model(test_)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หาก พิจารณา จาก การเข้าสู่ การ เป็น   AEC   ใน สิ้นปี นี้ แล้ว   ผม ว่า ความสามารถ ทาง ด้าน ภาษาอังกฤษ จะ ใช้ประโยชน์ ได้ มากกว่า คณิตศาสตร์ เยอะ มาก ครับ   ได้ ทั้ง ด้าน ติดต่อ ธุรกิจ    การท่องเที่ยว    การศึกษา\n",
      "[('หาก', 'B-p'), ('พิจารณา', 'I-p'), ('จาก', 'I-p'), ('การเข้าสู่', 'I-p'), ('การ', 'I-p'), ('เป็น', 'I-p'), (' ', 'I-p'), ('AEC', 'I-p'), (' ', 'I-p'), ('ใน', 'I-p'), ('สิ้นปี', 'I-p'), ('นี้', 'I-p'), ('แล้ว', 'I-p'), (' ', 'O'), ('ผม', 'B-c'), ('ว่า', 'I-c'), ('ความสามารถ', 'I-c'), ('ทาง', 'I-c'), ('ด้าน', 'I-c'), ('ภาษาอังกฤษ', 'I-c'), ('จะ', 'I-c'), ('ใช้ประโยชน์', 'I-c'), ('ได้', 'I-c'), ('มากกว่า', 'I-c'), ('คณิตศาสตร์', 'I-c'), ('เยอะ', 'I-c'), ('มาก', 'I-c'), ('ครับ', 'I-c'), (' ', 'O'), ('ได้', 'B-p'), ('ทั้ง', 'I-p'), ('ด้าน', 'I-p'), ('ติดต่อ', 'I-p'), ('ธุรกิจ', 'I-p'), ('  ', 'I-p'), ('การท่องเที่ยว', 'I-p'), ('  ', 'I-p'), ('การศึกษา', 'I-p')]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "test_pred = \" \".join(list(map(lambda word: word[0], test_sents[idx])))\n",
    "print(test_pred)\n",
    "print(test_sents[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.27s/it]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'หาก': 'B_C'}, {'พิจารณา': 'I_C'}, {'จาก': 'I_C'}, {'การเข้าสู่': 'I_C'}, {'การ': 'I_C'}, {'เป็น': 'I_C'}, {'AEC': 'I_C'}, {'ใน': 'I_C'}, {'สิ้นปี': 'I_C'}, {'นี้': 'I_C'}, {'แล้ว': 'I_C'}, {'ผม': 'B_C'}, {'ว่า': 'I_C'}, {'ความสามารถ': 'I_C'}, {'ทาง': 'I_C'}, {'ด้าน': 'I_P'}, {'ภาษาอังกฤษ': 'I_P'}, {'จะ': 'I_P'}, {'ใช้ประโยชน์': 'I_P'}, {'ได้': 'I_P'}, {'มากกว่า': 'I_P'}, {'คณิตศาสตร์': 'I_P'}, {'เยอะ': 'I_P'}, {'มาก': 'I_P'}, {'ครับ': 'I_P'}, {'ได้': 'I_P'}, {'ทั้ง': 'I_P'}, {'ด้าน': 'I_P'}, {'ติดต่อ': 'I_P'}, {'ธุรกิจ': 'I_P'}, {'การท่องเที่ยว': 'I_P'}, {'การศึกษา': 'I_P'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "predictions, raw_outputs = test_ner.predict([test_pred], split_on_space=True)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.28s/it]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ห': 'B_C'}, {'า': 'I_C'}, {'ก': 'O'}, {' ': 'I_C'}, {'พ': 'I_C'}, {'ิ': 'I_C'}, {'จ': 'I_C'}, {'า': 'I_C'}, {'ร': 'I_C'}, {'ณ': 'I_C'}, {'า': 'I_C'}, {' ': 'O'}, {'จ': 'I_C'}, {'า': 'I_C'}, {'ก': 'I_C'}, {' ': 'I_C'}, {'ก': 'I_C'}, {'า': 'I_C'}, {'ร': 'I_C'}, {'เ': 'I_C'}, {'ข': 'I_P'}, {'้': 'I_P'}, {'า': 'I_P'}, {'ส': 'I_P'}, {'ู': 'I_P'}, {'่': 'I_P'}, {' ': 'I_P'}, {'ก': 'I_P'}, {'า': 'I_P'}, {'ร': 'I_P'}, {' ': 'I_P'}, {'เ': 'I_P'}, {'ป': 'I_P'}, {'็': 'I_P'}, {'น': 'I_P'}, {' ': 'I_P'}, {' ': 'I_P'}, {' ': 'O'}, {'A': 'I_P'}, {'E': 'I_P'}, {'C': 'I_P'}, {' ': 'I_P'}, {' ': 'I_P'}, {' ': 'O'}, {'ใ': 'O'}, {'น': 'O'}, {' ': 'O'}, {'ส': 'O'}, {'ิ': 'O'}, {'้': 'O'}, {'น': 'O'}, {'ป': 'O'}, {'ี': 'O'}, {' ': 'O'}, {'น': 'O'}, {'ี': 'O'}, {'้': 'O'}, {' ': 'O'}, {'แ': 'O'}, {'ล': 'O'}, {'้': 'O'}, {'ว': 'O'}, {' ': 'O'}, {' ': 'O'}, {' ': 'O'}, {'ผ': 'O'}, {'ม': 'O'}, {' ': 'O'}, {'ว': 'O'}, {'่': 'O'}, {'า': 'O'}, {' ': 'O'}, {'ค': 'O'}, {'ว': 'O'}, {'า': 'O'}, {'ม': 'O'}, {'ส': 'O'}, {'า': 'O'}, {'ม': 'O'}, {'า': 'O'}, {'ร': 'O'}, {'ถ': 'O'}, {' ': 'O'}, {'ท': 'O'}, {'า': 'O'}, {'ง': 'O'}, {' ': 'O'}, {'ด': 'O'}, {'้': 'O'}, {'า': 'O'}, {'น': 'O'}, {' ': 'O'}, {'ภ': 'O'}, {'า': 'O'}, {'ษ': 'O'}, {'า': 'O'}, {'อ': 'O'}, {'ั': 'O'}, {'ง': 'O'}, {'ก': 'O'}, {'ฤ': 'O'}, {'ษ': 'O'}, {' ': 'O'}, {'จ': 'O'}, {'ะ': 'O'}, {' ': 'O'}, {'ใ': 'O'}, {'ช': 'O'}, {'้': 'O'}, {'ป': 'O'}, {'ร': 'O'}, {'ะ': 'O'}, {'โ': 'O'}, {'ย': 'O'}, {'ช': 'O'}, {'น': 'O'}, {'์': 'O'}, {' ': 'O'}, {'ไ': 'O'}, {'ด': 'O'}, {'้': 'O'}, {' ': 'O'}, {'ม': 'O'}, {'า': 'O'}, {'ก': 'O'}, {'ก': 'O'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "predictions, raw_outputs = test_ner.predict([test_pred], split_on_space=False)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'I_C', 'B_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "for sent in test_sents:\n",
    "    labels = []\n",
    "    for word, label in sent:\n",
    "        if word == ' ' or word == '  ':\n",
    "            continue;\n",
    "        label = label.upper().replace(\"-\", \"_\")\n",
    "        labels.append(label)\n",
    "    y_test.append(labels)\n",
    "    \n",
    "print(y_test[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n",
      "Running Prediction: 100%|██████████| 18/18 [00:02<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I_C', 'I_C', 'I_C', 'I_P', 'I_C', 'I_C', 'O', 'I_P', 'O', 'I_P', 'I_P', 'I_P', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'O', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'O', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'O', 'O', 'O', 'O', 'O', 'O', 'I_P', 'I_P', 'I_P', 'I_P', 'I_C']\n"
     ]
    }
   ],
   "source": [
    "test_list = []\n",
    "for sent in test_sents:\n",
    "    words = []\n",
    "    for word, label in sent:\n",
    "        words.append(word)\n",
    "    test_list.append(\" \".join(words))\n",
    "\n",
    "\n",
    "predictions, raw_outputs = test_ner.predict(test_list)\n",
    "\n",
    "y_pred = []\n",
    "for preds in predictions:\n",
    "    y_pred.append([list(pred.items())[0][1] for pred in preds])\n",
    "\n",
    "print(y_pred[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = []\n",
    "y_test_ = []\n",
    "for i in range(len(y_test)):\n",
    "    if len(y_pred[i]) != len(y_test[i]):\n",
    "        continue;\n",
    "    y_pred_.append(y_pred[i])\n",
    "    y_test_.append(y_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5009717123731375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          _C       0.00      0.00      0.00       128\n",
      "          _P       0.01      0.05      0.01       128\n",
      "\n",
      "   micro avg       0.01      0.02      0.01       256\n",
      "   macro avg       0.00      0.02      0.01       256\n",
      "weighted avg       0.00      0.02      0.01       256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "print(\"accuracy:\" ,accuracy_score(y_test_, y_pred_))\n",
    "print(classification_report(y_test_, y_pred_))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6dc3a5d5e5e7987cc7d18355d6f408e2f42f41b0aca7d07c9009e9d98d8dd16"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('argument')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
