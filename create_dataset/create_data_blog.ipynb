{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36a1c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ace0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#จัดการประโยคซ้ำ\n",
    "data_not=[]\n",
    "def Unique(p):\n",
    "    text=re.sub(\"<^[>]*>\",\"\",p)\n",
    "    text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    "    text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    "    if text not in data_not:\n",
    "        data_not.append(text)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "    \"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    str_line = \"\"\n",
    "    with open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            if(line == \"\\n\"):\n",
    "                lines.append(str_line)\n",
    "                lines[-1] = lines[-1].replace(\"\\n\",\" \")\n",
    "                str_line = \"\"\n",
    "            else:\n",
    "                str_line += line\n",
    "        lines.append(str_line.replace(\"\\n\",\" \"))\n",
    "        lines = \"\\n\".join(lines)\n",
    "        return lines\n",
    "#     return [a for a in lines if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in lista:\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll\n",
    "    \n",
    "\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('<newline>') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'<newline>'  #เราจะไปเดินเล่นที่\\tO\\nหนองคาย\\tB-location\\n\n",
    "        i+=1\n",
    "    return text\n",
    "\n",
    "\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    "    text=text.strip()\n",
    "    text=re.sub(\"<[^>]*>\",\"\",text)\n",
    "    text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')#  ตัดการกับพวกไม่มี tag word\n",
    "    text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    "    text2=[]\n",
    "    for i in text.split('***'):\n",
    "        if \"[\" in i:\n",
    "            text2.append(i)\n",
    "        else:\n",
    "            text2.append(\"[word]\"+i+\"[/word]\")\n",
    "    text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    "    return text.replace(\"[word][/word]\",\"\")\n",
    "\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text) # นำไปใส่ tag [word]\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text) # แยก tag ออกมาจากข้อความ\n",
    "    j=0\n",
    "    conll2002=\"\" # ประกาศตัวแปรเก็บ conll2002\n",
    "    for tagopen,text,tagclose in tag: # ลูปใน tag โดยเป็น (tagopen,text,tagclose)\n",
    "        word_cut=word_tokenize(text,engine=\"newmm\") # ใช้ตัวตัดคำ newmm ของ PyThaiNLP\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut): #ลูปตามจำนวน token ที่ตัดในtag\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word': # ไม่เป็น tag [word] และเป็น i หรือตัวเริ่มต้น tag\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen  \n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen \n",
    "            else: # เป็น [word]\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='<newline>' #ลาว\\t-location\\n\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002) # เพิ่ม postag ใส่\n",
    "\n",
    "def alldata_list(lists, head=False, pos=True):\n",
    "    get_tile = 0 if head == True else 1\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data = data.split(\"\\n\")[get_tile:]\n",
    "        data = \" \".join(data)\n",
    "        data_num=[]\n",
    "        txt=text2conll2002(data,pos=pos).split('<newline>') # นำไปแปลงเป็น conll2002\n",
    "        for d in txt:\n",
    "            tt=d.split('\\t')\n",
    "            if d!=\"\":\n",
    "                if len(tt)==3:\n",
    "                    data_num.append((tt[0],tt[1],tt[2])) \n",
    "                else:\n",
    "                    data_num.append((tt[0],tt[1]))\n",
    "        #print(data_num)\n",
    "        data_all.append(data_num)\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "140056f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/LAW/\"\n",
    "data1  = []\n",
    "for name_folder in os.listdir(path):\n",
    "    for f_name in os.listdir(path+name_folder):\n",
    "        data1.append(get_data(path+name_folder+\"/\"+f_name))\n",
    "\n",
    "datatofile_ = alldata_list(data1, head=False, pos=False)\n",
    "datatofile_pos = alldata_list(data1, head=False, pos=True) # นำไปผ่านขั้นตอน 1 2 3 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6484c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_empty_list(datatofile):\n",
    "    temp_data = []\n",
    "    for data in datatofile:\n",
    "        if data != []:\n",
    "            temp_data.append(data)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "677c822b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datatofile_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b15f2a",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "572849c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"blog\"\n",
    "\n",
    "with open(path+file_name+\"-pos.conll\",\"w\", encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile_pos):\n",
    "        for j in datatofile_pos[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile_pos):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1\n",
    "\n",
    "with open(path+file_name+\".conll\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile_):\n",
    "        for j in datatofile_[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\n\")\n",
    "        if i+1<len(datatofile_):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d7fb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/data/\"\n",
    "\n",
    "pickle.dump(datatofile_pos, open(path+\"blog-pos.data\", 'wb'))\n",
    "pickle.dump(datatofile_, open(path+\"blog.data\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fda66",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07ad1ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dataset/data/\"\n",
    "\n",
    "with open(path+\"blog-pos.data\", 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "len(datatofile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
