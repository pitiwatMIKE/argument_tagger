{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6beaa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c5c7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#จัดการประโยคซ้ำ\n",
    "def Unique(p):\n",
    "    text=re.sub(\"<^[>]*>\",\"\",p)\n",
    "    text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    "    text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    "    if text not in data_not:\n",
    "        data_not.append(text)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "    \"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    str_line = \"\"\n",
    "    with open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            if (i > 1):\n",
    "                if(line == \"\\n\"):\n",
    "                    lines.append(str_line)\n",
    "                    lines[-1] = lines[-1].replace(\"\\n\",\" \")\n",
    "                    str_line = \"\"\n",
    "                else:\n",
    "                    str_line += line\n",
    "        lines.append(str_line.replace(\"\\n\",\" \"))\n",
    "    return [a for a in lines if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in lista:\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll\n",
    "    \n",
    "\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'  #เราจะไปเดินเล่นที่\\tO\\nหนองคาย\\tB-location\\n\n",
    "        i+=1\n",
    "    return text\n",
    "\n",
    "\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    "    text=text.strip()\n",
    "    text=re.sub(\"<[^>]*>\",\"\",text)\n",
    "    text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')#  ตัดการกับพวกไม่มี tag word\n",
    "    text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    "    text2=[]\n",
    "    for i in text.split('***'):\n",
    "        if \"[\" in i:\n",
    "            text2.append(i)\n",
    "        else:\n",
    "            text2.append(\"[word]\"+i+\"[/word]\")\n",
    "    text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    "    return text.replace(\"[word][/word]\",\"\")\n",
    "\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text) # นำไปใส่ tag [word]\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text) # แยก tag ออกมาจากข้อความ\n",
    "    j=0\n",
    "    conll2002=\"\" # ประกาศตัวแปรเก็บ conll2002\n",
    "    for tagopen,text,tagclose in tag: # ลูปใน tag โดยเป็น (tagopen,text,tagclose)\n",
    "        word_cut=word_tokenize(text,engine=\"newmm\") # ใช้ตัวตัดคำ newmm ของ PyThaiNLP\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut): #ลูปตามจำนวน token ที่ตัดในtag\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word': # ไม่เป็น tag [word] และเป็น i หรือตัวเริ่มต้น tag\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen \n",
    "            else: # เป็น [word]\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n' #ลาว\\t-location\\n\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002) # เพิ่ม postag ใส่\n",
    "\n",
    "def alldata_list(lists, pos = True):\n",
    "    data_all=[]\n",
    "    for data in lists:        \n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=pos).split('\\n') # นำไปแปลงเป็น conll2002\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2])) \n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "    #print(data_all)\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fc72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/LAW/\"\n",
    "data_not=[]\n",
    "data1  = []\n",
    "\n",
    "for name_folder in os.listdir(path):\n",
    "    for f_name in os.listdir(path+name_folder):\n",
    "        data1 += getall(get_data(path+name_folder+\"/\"+f_name))\n",
    "        \n",
    "datatofile_ = alldata_list(data1, pos=False)\n",
    "datatofile_pos = alldata_list(data1, pos=True) # นำไปผ่านขั้นตอน 1 2 3 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df93c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_empty_list(datatofile):\n",
    "    temp_data = []\n",
    "    for data in datatofile:\n",
    "        if data != []:\n",
    "            temp_data.append(data)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6341fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatofile_ = clear_empty_list(datatofile_)\n",
    "datatofile_pos = clear_empty_list(datatofile_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd4098",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27956a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"comment\"\n",
    "\n",
    "with open(path+file_name+\"-pos.conll\",\"w\", encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile_pos):\n",
    "        for j in datatofile_pos[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile_pos):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1\n",
    "\n",
    "with open(path+file_name+\".conll\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile_):\n",
    "        for j in datatofile_[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\n\")\n",
    "        if i+1<len(datatofile_):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "af31b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/data/\"\n",
    "\n",
    "pickle.dump(datatofile_pos, open(path+\"comment-pos.data\", 'wb'))\n",
    "pickle.dump(datatofile_, open(path+\"comment.data\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e82bb",
   "metadata": {},
   "source": [
    "# save data for train bert lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a470aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile_:\n",
    "    text_inside = []\n",
    "    for word, label in data:\n",
    "        if word.strip() == '':\n",
    "            text_inside.append(('_', label))\n",
    "        else:\n",
    "            text_inside.append((word, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42e6cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format_label(datatofile):\n",
    "    data_file_bert = []\n",
    "    for data in datatofile:\n",
    "        list_word_lable = []\n",
    "        for word_label in data:\n",
    "            word = word_label[0]\n",
    "            label = word_label[1]\n",
    "            label = label.upper().replace(\"-\", \"_\")\n",
    "            list_word_lable.append((word, label))\n",
    "        data_file_bert.append(list_word_lable)\n",
    "    return data_file_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd3a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('อะไหล่', 'B_C'), ('เทอร์โบ', 'I_C'), ('_', 'I_C'), ('อี', 'I_C'), ('ซุ', 'I_C'), ('สุ', 'I_C'), ('_', 'I_C'), ('[', 'I_C'), ('c', 'I_C'), (']', 'I_C'), ('_', 'I_C'), ('[', 'I_C'), ('p', 'I_C'), (']ทน', 'I_C'), ('และ', 'I_C'), ('ราคา', 'I_C'), ('ถูก', 'I_C'), ('กว่า', 'I_C'), ('_', 'I_C')]\n"
     ]
    }
   ],
   "source": [
    "data_train = convert_format_label(train_sents)\n",
    "data_test = convert_format_label(test_sents)\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a421adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"train\"\n",
    "\n",
    "with open(path+file_name+\"_CONLL_BERT.txt\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(data_train):\n",
    "        for j in data_train[i]:\n",
    "            f.write(j[0]+\" \"+j[1]+\"\\n\")\n",
    "        if i+1<len(data_train):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f408fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"test\"\n",
    "\n",
    "with open(path+file_name+\"_CONLL_BERT.txt\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(data_test):\n",
    "        for j in data_test[i]:\n",
    "            f.write(j[0]+\" \"+j[1]+\"\\n\")\n",
    "        if i+1<len(data_test):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245d8de",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b318c531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dataset/data/\"\n",
    "\n",
    "with open(path+\"comment-pos.data\", 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "len(datatofile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
