{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7f94fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / Load File\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "# Plot Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Report\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "\n",
    "# Load Vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utility\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Model Utility\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82e26a",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50ee4cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n",
      "[('ถ้า', 'B-c'), ('เดินทาง', 'I-c'), ('กลางคืน', 'I-c'), ('ก็', 'I-c'), ('รถทัวร์', 'I-c'), ('ครับ', 'I-c'), (' ', 'I-c'), (' ', 'O'), ('เพราะ', 'B-p'), ('รถ', 'I-p'), ('ไม่', 'I-p'), ('เยอะ', 'I-p'), (' ', 'I-p'), ('ความเสี่ยง', 'I-p'), ('การ', 'I-p'), ('เกิด', 'I-p'), ('อุบัติ', 'I-p'), ('ห', 'I-p'), ('ตุ', 'I-p'), ('ก็', 'I-p'), ('น้อย', 'I-p'), ('(', 'I-p'), ('มั้ง', 'I-p'), (')', 'I-p'), (' ', 'I-p'), (' ', 'O'), ('ถ้า', 'B-c'), ('กลางวัน', 'I-c'), ('ก็', 'I-c'), ('เครื่องบิน', 'I-c'), ('ครับ', 'I-c'), (' ', 'I-c'), (' ', 'O'), ('เพราะ', 'B-p'), (' ', 'I-p'), ('มัน', 'I-p'), ('ใช้เวลา', 'I-p'), ('น้อย', 'I-p'), ('จะ', 'I-p'), ('ได้', 'I-p'), ('มี', 'I-p'), ('เวลา', 'I-p'), ('ระหว่าง', 'I-p'), ('วัน', 'I-p'), ('เยอะ', 'I-p'), ('ๆ', 'I-p'), (' ', 'I-p')]\n"
     ]
    }
   ],
   "source": [
    "path_name = \"../../dataset/data/\"\n",
    "\n",
    "with open(path_name + 'comment-pos.data', 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile:\n",
    "    text_inside = []\n",
    "    for word, pos, label in data:\n",
    "        text_inside.append((word, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1d8a4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55677, 400)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai2fit_model = KeyedVectors.load_word2vec_format('../../thai2vec/thai2vecNoSym.bin', binary=True)\n",
    "thai2fit_weight = thai2fit_model.vectors\n",
    "thai2fit_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0d9fb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_word:  4542\n",
      "n_tag:  6\n",
      "n_thai2dict 55677\n",
      "{'B-c': 0, 'B-p': 1, 'I-c': 2, 'I-p': 3, 'O': 4, 'pad': 5}\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "ner_list = []\n",
    "thai2dict = {}\n",
    "\n",
    "for sent in train_sents:\n",
    "    for word, label in sent:\n",
    "        word_list.append(word)\n",
    "        ner_list.append(label)\n",
    "\n",
    "for word in thai2fit_model.index_to_key:\n",
    "    thai2dict[word] = thai2fit_model[word]\n",
    "\n",
    "word_list.append(\"pad\")\n",
    "word_list.append(\"unknown\") #Special Token for Unknown words (\"UNK\")\n",
    "ner_list.append(\"pad\")\n",
    "\n",
    "all_word = sorted(set(word_list))\n",
    "all_ner = sorted(set(ner_list))\n",
    "all_thai2dict = sorted(set(thai2dict))\n",
    "\n",
    "word_to_idx = dict((word, i) for i, word in enumerate(all_word)) #convert word to index\n",
    "ner_to_idx = dict((label, i) for i, label in enumerate(all_ner)) #convert ner(label) to index\n",
    "thai2dict_to_idx = dict((word, i) for i, word in enumerate(thai2dict)) #convert thai2fit to index\n",
    "\n",
    "idx_to_word = dict((idx, word) for word, idx in word_to_idx.items()) #convert index to word\n",
    "idx_to_ner = dict((idx,label) for label, idx in ner_to_idx.items()) #convert index to ner(label)\n",
    "idx_to_thai2fit = dict((idx,word) for word, idx in thai2dict_to_idx.items())# convert index to thai2fit\n",
    "\n",
    "n_word = len(word_to_idx)\n",
    "n_tag = len(ner_to_idx)\n",
    "n_thai2dict = len(thai2dict_to_idx)\n",
    "print(\"n_word: \", n_word)\n",
    "print(\"n_tag: \", n_tag)\n",
    "print(\"n_thai2dict\", n_thai2dict)\n",
    "print(ner_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8350850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in thai2dict for w_i in w])\n",
    "char2idx = {c: i + 5 for i, c in enumerate(chars)}\n",
    "\n",
    "char2idx[\"pad\"] = 0\n",
    "char2idx[\"unknown\"] = 1\n",
    "char2idx[\" \"] = 2\n",
    "\n",
    "char2idx[\"$\"] = 3\n",
    "char2idx[\"#\"] = 4\n",
    "char2idx[\"!\"] = 5\n",
    "char2idx[\"%\"] = 6\n",
    "char2idx[\"&\"] = 7\n",
    "char2idx[\"*\"] = 8\n",
    "char2idx[\"+\"] = 9\n",
    "char2idx[\",\"] = 10\n",
    "char2idx[\"-\"] = 11\n",
    "char2idx[\".\"] = 12\n",
    "char2idx[\"/\"] = 13\n",
    "char2idx[\":\"] = 14\n",
    "char2idx[\";\"] = 15\n",
    "char2idx[\"?\"] = 16\n",
    "char2idx[\"@\"] = 17\n",
    "char2idx[\"^\"] = 18\n",
    "char2idx[\"_\"] = 19\n",
    "char2idx[\"`\"] = 20\n",
    "char2idx[\"=\"] = 21\n",
    "char2idx[\"|\"] = 22\n",
    "char2idx[\"~\"] = 23\n",
    "char2idx[\"'\"] = 24\n",
    "char2idx['\"'] = 25\n",
    "\n",
    "char2idx[\"(\"] = 26\n",
    "char2idx[\")\"] = 27\n",
    "char2idx[\"{\"] = 28\n",
    "char2idx[\"}\"] = 29\n",
    "char2idx[\"<\"] = 30\n",
    "char2idx[\">\"] = 31\n",
    "char2idx[\"[\"] = 32\n",
    "char2idx[\"]\"] = 33\n",
    "\n",
    "n_chars = len(char2idx)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14b2c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 400\n",
    "max_len_char = 32\n",
    "\n",
    "character_LSTM_unit = 32\n",
    "char_embedding_dim = 32\n",
    "main_lstm_unit = 256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17539f44",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b067f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_word(list_sent):\n",
    "    idxs = list()\n",
    "    for word in list_sent:\n",
    "        if word in thai2dict:\n",
    "            idxs.append(thai2dict_to_idx[word])\n",
    "        else:\n",
    "            idxs.append(thai2dict_to_idx[\"unknown\"]) #Use UNK tag for unknown word\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence_target(input_label):\n",
    "    idxs = [ner_to_idx[BIO] for BIO in input_label]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20d33392",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent =[ [ word for word, label in sent]for sent in train_sents ] #words only\n",
    "train_targets =[ [ label for word, label in sent]for sent in train_sents ] #NER only\n",
    "\n",
    "input_test_sent =[ [ word for word, label in sent]for sent in test_sents ] #words only\n",
    "test_targets =[ [ label for word, label in sent]for sent in test_sents ] #NER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f2fc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word Trainig\n",
    "X_word_train = [prepare_sequence_word(list_sent) for list_sent in input_sent]\n",
    "X_word_train = pad_sequences(maxlen=max_len, sequences=X_word_train, value=thai2dict_to_idx[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## character Training\n",
    "X_char_train = []\n",
    "for sentence in train_sents:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if sentence[i][0][j] in char2idx:\n",
    "                    word_seq.append(char2idx[sentence[i][0][j]])\n",
    "                else:\n",
    "                    word_seq.append(char2idx[\"unknown\"])\n",
    "            except:\n",
    "                word_seq.append(char2idx[\"pad\"])\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_train.append(np.array(sent_seq))\n",
    "\n",
    "\n",
    "y_train = [prepare_sequence_target(labels_sent) for labels_sent in train_targets ]\n",
    "y_train = pad_sequences(maxlen=max_len, sequences=y_train, value=ner_to_idx[\"pad\"], padding='post', truncating='post')\n",
    "y_train = [to_categorical(idx_ner, num_classes=n_tag) for idx_ner in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07e7cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (552, 400)\n",
      "y_train:  (552, 400, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: \",X_word_train.shape)\n",
    "# print(\"y_tain: \",len(y_train), \"=>\", y_train[0].shape)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "print(\"y_train: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed905008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word Testing\n",
    "X_word_test = [prepare_sequence_word(list_sent) for list_sent in input_test_sent]\n",
    "X_word_test = pad_sequences(maxlen=max_len, sequences=X_word_test, value=thai2dict_to_idx[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## character Training\n",
    "X_char_test = []\n",
    "for sentence in test_sents:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if sentence[i][0][j] in char2idx:\n",
    "                    word_seq.append(char2idx[sentence[i][0][j]])\n",
    "                else:\n",
    "                    word_seq.append(char2idx[\"unknown\"])\n",
    "            except:\n",
    "                word_seq.append(char2idx[\"pad\"])\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_test.append(np.array(sent_seq))\n",
    "\n",
    "\n",
    "y_test = [prepare_sequence_target(labels_sent) for labels_sent in test_targets ]\n",
    "y_test = pad_sequences(maxlen=max_len, sequences=y_test, value=ner_to_idx[\"pad\"], padding='post', truncating='post')\n",
    "y_test = [to_categorical(idx_ner, num_classes=n_tag) for idx_ner in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "281871e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (138, 400)\n",
      "y_test:  (138, 400, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: \",X_word_test.shape)\n",
    "# print(\"y_: \",len(y_test), \"=>\", y_test[0].shape)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe37b6",
   "metadata": {},
   "source": [
    "# train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b9e8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input ,LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from tensorflow.keras.layers import concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8ea9135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         [(None, 400, 32)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_ (InputLayer)        [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 400, 32, 32)  12768       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 400, 400)     22270800    word_input_[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 400, 32)      8320        time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400, 432)     0           word_embedding[0][0]             \n",
      "                                                                 time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 400, 432)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 400, 256)     705536      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 400, 100)     25700       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400, 6)       606         time_distributed_5[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 23,023,730\n",
      "Trainable params: 752,930\n",
      "Non-trainable params: 22,270,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#word Input\n",
    "word_in = Input(shape=(max_len), name='word_input_')\n",
    "\n",
    "#word Enbedding Using Thai2Fit\n",
    "word_embeddings = Embedding(input_dim=n_thai2dict, output_dim=400, weights = [thai2fit_weight], input_length=max_len,\n",
    "                                               mask_zero=False, trainable=False, name=\"word_embedding\")(word_in)\n",
    "\n",
    "# Character Input\n",
    "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
    "\n",
    "# Character Embedding\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
    "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
    "\n",
    "# Character Sequence to Vector via BiLSTM\n",
    "char_enc = TimeDistributed(LSTM(units=character_LSTM_unit, return_sequences=False))(emb_char)\n",
    "\n",
    "# Concatenate All Embedding\n",
    "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
    "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
    "\n",
    "main_lstm = LSTM(units=main_lstm_unit, return_sequences=True,)(all_word_embeddings)\n",
    "dens = TimeDistributed(Dense(100, activation=\"relu\"))(main_lstm)\n",
    "out = Dense(n_tag, activation=\"softmax\")(dens)\n",
    "model = keras.Model(inputs=[word_in, char_in], outputs=[out])\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b7c7180",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "9/9 [==============================] - 5s 331ms/step - loss: 0.7411 - val_loss: 0.3167\n",
      "Epoch 2/40\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.2661 - val_loss: 0.2271\n",
      "Epoch 3/40\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.2229 - val_loss: 0.2045\n",
      "Epoch 4/40\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 0.2083 - val_loss: 0.1926\n",
      "Epoch 5/40\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.1999 - val_loss: 0.1921\n",
      "Epoch 6/40\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 0.1986 - val_loss: 0.1870\n",
      "Epoch 7/40\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.1931 - val_loss: 0.1822\n",
      "Epoch 8/40\n",
      "9/9 [==============================] - 2s 277ms/step - loss: 0.1905 - val_loss: 0.1799\n",
      "Epoch 9/40\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.1852 - val_loss: 0.1785\n",
      "Epoch 10/40\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.1847 - val_loss: 0.1750\n",
      "Epoch 11/40\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.1799 - val_loss: 0.1753\n",
      "Epoch 12/40\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.1850 - val_loss: 0.1699\n",
      "Epoch 13/40\n",
      "9/9 [==============================] - 2s 264ms/step - loss: 0.1773 - val_loss: 0.1684\n",
      "Epoch 14/40\n",
      "9/9 [==============================] - 2s 264ms/step - loss: 0.1786 - val_loss: 0.1661\n",
      "Epoch 15/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1746 - val_loss: 0.1645\n",
      "Epoch 16/40\n",
      "9/9 [==============================] - 2s 264ms/step - loss: 0.1778 - val_loss: 0.1642\n",
      "Epoch 17/40\n",
      "9/9 [==============================] - 2s 266ms/step - loss: 0.1746 - val_loss: 0.1596\n",
      "Epoch 18/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1703 - val_loss: 0.1565\n",
      "Epoch 19/40\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 0.1688 - val_loss: 0.1662\n",
      "Epoch 20/40\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 0.1734 - val_loss: 0.1566\n",
      "Epoch 21/40\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.1701 - val_loss: 0.1581\n",
      "Epoch 22/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1657 - val_loss: 0.1578\n",
      "Epoch 23/40\n",
      "9/9 [==============================] - 2s 266ms/step - loss: 0.1645 - val_loss: 0.1553\n",
      "Epoch 24/40\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 0.1624 - val_loss: 0.1526\n",
      "Epoch 25/40\n",
      "9/9 [==============================] - 2s 273ms/step - loss: 0.1610 - val_loss: 0.1490\n",
      "Epoch 26/40\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 0.1596 - val_loss: 0.1498\n",
      "Epoch 27/40\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.1596 - val_loss: 0.1467\n",
      "Epoch 28/40\n",
      "9/9 [==============================] - 2s 273ms/step - loss: 0.1542 - val_loss: 0.1454\n",
      "Epoch 29/40\n",
      "9/9 [==============================] - 2s 266ms/step - loss: 0.1585 - val_loss: 0.1479\n",
      "Epoch 30/40\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.1629 - val_loss: 0.1503\n",
      "Epoch 31/40\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 0.1566 - val_loss: 0.1469\n",
      "Epoch 32/40\n",
      "9/9 [==============================] - 2s 266ms/step - loss: 0.1502 - val_loss: 0.1467\n",
      "Epoch 33/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1501 - val_loss: 0.1579\n",
      "Epoch 34/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1485 - val_loss: 0.1444\n",
      "Epoch 35/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1491 - val_loss: 0.1580\n",
      "Epoch 36/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1542 - val_loss: 0.1540\n",
      "Epoch 37/40\n",
      "9/9 [==============================] - 2s 268ms/step - loss: 0.1485 - val_loss: 0.1461\n",
      "Epoch 38/40\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.1480 - val_loss: 0.1442\n",
      "Epoch 39/40\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 0.1477 - val_loss: 0.1466\n",
      "Epoch 40/40\n",
      "9/9 [==============================] - 2s 269ms/step - loss: 0.1415 - val_loss: 0.1441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f44d0fc670>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_word_train, np.array(X_char_train)], y_train, batch_size=64, epochs=40, verbose=1,  validation_data=([X_word_test, np.array(X_char_test)], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9ba6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = model.predict([X_word_test,np.array(X_char_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49711444",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(0,len(pred_model)):\n",
    "    out = np.argmax(pred_model[i], axis=-1)\n",
    "    true = np.argmax(y_test[i], axis=-1)\n",
    "    revert_pred=[idx_to_ner[i] for i in out]\n",
    "    revert_true=[idx_to_ner[i] for i in true]\n",
    "    y_pred.append(revert_pred)\n",
    "    y_true.append(revert_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20b83a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = [[y for y in tag if y != \"pad\"]for tag in y_true]\n",
    "y_hat = [[y_ for y_ in tag if y_ != \"pad\"]for tag in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b03c534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6076181734740707\n",
      "['B-c', 'B-p', 'I-c', 'I-p']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         B-c       0.82      0.20      0.32       157\n",
      "         B-p       0.75      0.48      0.59       158\n",
      "         I-c       0.58      0.51      0.54      1735\n",
      "         I-p       0.60      0.81      0.69      3727\n",
      "\n",
      "   micro avg       0.60      0.70      0.64      5777\n",
      "   macro avg       0.69      0.50      0.53      5777\n",
      "weighted avg       0.60      0.70      0.63      5777\n",
      " samples avg       0.46      0.46      0.46      5777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def ner_classification_report(y_true, y_pred):\n",
    " \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "    print(\"accuracy\",accuracy_score(y_true_combined, y_pred_combined))\n",
    "    tagset = list(sorted(set(lb.classes_)))\n",
    "    tagset = tagset[:-1]\n",
    "    print(tagset)\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "        zero_division=0\n",
    "    )\n",
    "  \n",
    "print(ner_classification_report(y_real,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7affe6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6076181734740707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           c       0.13      0.17      0.15       157\n",
      "           p       0.11      0.25      0.16       158\n",
      "\n",
      "   micro avg       0.12      0.21      0.15       315\n",
      "   macro avg       0.12      0.21      0.15       315\n",
      "weighted avg       0.12      0.21      0.15       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "print(\"accuracy:\" ,accuracy_score(y_real, y_hat))\n",
    "print(classification_report(y_real, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5e18e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../trained_model/LSTM/\"\n",
    "\n",
    "with open(path+'chardict.pickle', 'wb') as chardict:\n",
    "    pickle.dump(char2idx, chardict)\n",
    "    \n",
    "with open(path+'nerdict.pickle', 'wb') as nerdict:\n",
    "    pickle.dump(ner_to_idx, nerdict)\n",
    "    \n",
    "model.save_weights(path+\"model_LSTM.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
